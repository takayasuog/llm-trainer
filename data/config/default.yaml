# model/data params
# base_model: "rinna/japanese-gpt-neox-3.6b-instruction-sft" # the only required argument
base_model: "cyberagent/open-calm-7b"
data_path: "/home/user0/work/data/datasets/custom_datasets"
output_dir: "/home/user0/work/data/trained/open-calm-7b_customdataset"
model_cache_dir: "/home/user0/work/data/model"

# training hyperparams
local_rank: 0
num_of_gpus: 1
batch_size: 128
micro_batch_size: 16
num_epochs: 10
learning_rate: 3.0e-4
cutoff_len: 256
val_set_size: 2000
max_memory_MB: 15000

# lora hyperparams
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: []
train_on_inputs: True  # if False, masks out inputs in loss
add_eos_token: False
group_by_length: False  # faster, but produces an odd training loss curve
resume_from_checkpoint: ""  # either training checkpoint or final adapter
prompt_template_name: "alpaca_ja"  # Prompt template to use, default to Alpaca
prompt_template_dir_path: "/home/user0/work/data/prompt"  # Prompt template to use, default to Alpaca

# wandb params
wandb_run_name: "mywandb"

# deepspeed params
deepspeed_enabled: False
stage: 3
enable_hybrid_engine: False
inference_tp_size: 1
release_inference_cache: False
pin_parameters: True
tp_gather_partition_size: 8
max_out_tokens: 512
