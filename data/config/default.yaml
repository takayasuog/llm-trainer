# model/data params
base_model: "rinna/japanese-gpt-neox-3.6b-instruction-sft" # the only required argument
data_path: "/home/user0/work/data/datasets/alpaca_ja/alpaca_cleaned_ja.json"
output_dir: "./lora-alpaca"
model_cache_dir: "/home/user0/work/data/model"

# training hyperparams
local_rank: 0
num_of_gpus: 1
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 3e-4
cutoff_len: 256
val_set_size: 2000

# lora hyperparams
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules:
  - "query_key_value"
  - "xxx"
train_on_inputs: True  # if False, masks out inputs in loss
add_eos_token: False
group_by_length: False  # faster, but produces an odd training loss curve
resume_from_checkpoint: ""  # either training checkpoint or final adapter
prompt_template_name: "alpaca"  # Prompt template to use, default to Alpaca
prompt_template_dir_path: "/home/user0/work/data/prompt"  # Prompt template to use, default to Alpaca

# wandb params
wandb_run_name: "mywandb"