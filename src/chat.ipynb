{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKNIZER =\"cyberagent/open-calm-7b\"\n",
    "CAUSAL_LM=\"cyberagent/open-calm-7b\"\n",
    "\n",
    "from configs.finetune_config import FinetuneConfig\n",
    "conf_path=\"/home/user0/work/data/config/default.yaml\"\n",
    "conf: FinetuneConfig = FinetuneConfig.load(conf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    GPTNeoXTokenizerFast,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "\n",
    "compute_dtype = torch.bfloat16\n",
    "quantization_config=BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_quant_type=\"nf4\", # {'fp4', 'nf4'}\n",
    ")\n",
    "\n",
    "# トークナイザーとモデルの準備\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        conf.base_model,\n",
    "        use_fast=False,\n",
    "        cache_dir=conf.model_cache_dir,\n",
    "    )\n",
    "except ValueError as e:\n",
    "    model_config = AutoConfig.from_pretrained(conf.base_model)\n",
    "    if model_config.model_type == \"gpt_neox\":\n",
    "        tokenizer = GPTNeoXTokenizerFast.from_pretrained(\n",
    "            conf.base_model,\n",
    "            use_fast=False,\n",
    "            cache_dir=conf.model_cache_dir,\n",
    "        )\n",
    "    else:\n",
    "        raisetokenizer = AutoTokenizer.from_pretrained(\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    CAUSAL_LM,\n",
    "    device_map=\"auto\", \n",
    "    torch_dtype=compute_dtype,\n",
    "    quantization_config=quantization_config,\n",
    "    cache_dir=conf.model_cache_dir,\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, conf.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.prompter import Prompter\n",
    "prompter:Prompter = Prompter(conf.prompt_template_name, conf.prompt_template_dir_path)\n",
    "def generate_prompt(prompt: str):\n",
    "    return prompter.generate_prompt(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_prefix=\"ユーザー: あなたは『アルパ』という女の子になって受け答えをしてください。<NL>\"\n",
    "def talk(prompt):\n",
    "    prompt = generate_prompt(prompt)\n",
    "    token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "      # output_ids = model.generate(\n",
    "      #     # token_ids.to(model.device),\n",
    "      #     token_ids,\n",
    "      #     do_sample=True,\n",
    "      #     max_new_tokens=64,\n",
    "      #     temperature=0.7,\n",
    "      #     pad_token_id=tokenizer.pad_token_id,\n",
    "      #     bos_token_id=tokenizer.bos_token_id,\n",
    "      #     eos_token_id=tokenizer.eos_token_id)\n",
    "      output_ids = model.generate(\n",
    "        input_ids=token_ids,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        no_repeat_ngram_size=2,\n",
    "      )\n",
    "    output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):])\n",
    "    output = output.replace(\"<NL>\", \"\\n\")\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "talk(\"こんにちは！今日は暑かったね。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
